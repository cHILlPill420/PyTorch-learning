{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient calculation with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 \n",
    "Here\n",
    "- requires_grad = True: to optimize the tensor variable (tells pytorch that it needs to calculate gradient wrt this tensor later in  optimization steps)\n",
    "And\n",
    "- grad_fn =: tells pytorch to backpropagate later:  \n",
    "    - for add function (since y=x+3 is add function)[ AddBackward0 dy/dx]\n",
    "    - for multiplication function (since z= x*3) [MulBackward0 dz/dx]\n",
    "    - for mean function ( w= z.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9075,  1.5435, -0.3865], requires_grad=True)\n",
      "tensor([2.0925, 4.5435, 2.6135], grad_fn=<AddBackward0>)\n",
      "tensor([-2.7224,  4.6304, -1.1594], grad_fn=<MulBackward0>)\n",
      "tensor([-2.7224,  4.6304, -1.1594], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y= x+3\n",
    "print(y)\n",
    "z= x*3\n",
    "print(z)\n",
    "w = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Calculating grad_fn ( gradient function) wrt input \n",
    "- input: x\n",
    "- outputs : y,z,w\n",
    "- gradient function( dz/dx | dy/dx | dw/dx) can only be used if the function is directly or in directly corelated with input ( where required_grad= True)\n",
    "- .backward(): creates a vector Jacobian product (chain rule) to get the gradients (ie. this function calculates dw/dx then x will store its gradient at x.grad)\n",
    "- Jacobian product = JM * GV\n",
    "    - JM ( Jacobian matrix) : elements -> partial derivatives of each output(y) wrt input(x)\n",
    "    - GV ( Gradient vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#2.2.1 grad for scalar outputs(w [since only one element])\n",
    "w.backward() \n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2000, 2.0023, 1.0012])\n"
     ]
    }
   ],
   "source": [
    "#2.2.2 grad for vector outputs (like y and z)\n",
    "v= torch.tensor([0.2, 1.0023, 0.00124], dtype= torch.float32) #defining gradient vector [since y is not scalar]\n",
    "y.backward(v) #must pass gradient vector as arguement\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 prevent creating gradient function (grad_fn) and  stopping from tracking history from computational graph\n",
    "- Three ways:\n",
    "    - x.requires_grad_(False) \n",
    "    - x.detach()\n",
    "    - with torch.no_grad():\n",
    "        #operations\n",
    "- [x = tensor requiring gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9887,  0.5679, -0.5729], requires_grad=True)\n",
      "tensor([ 0.9887,  0.5679, -0.5729])\n"
     ]
    }
   ],
   "source": [
    "#2.3.1 .requires_grad_(False)\n",
    "q= torch.randn(3, requires_grad= True) #tensor requiring gradient\n",
    "print(q)\n",
    "q.requires_grad_(False)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3815,  0.1716,  1.0518], requires_grad=True)\n",
      "tensor([-1.3815,  0.1716,  1.0518])\n"
     ]
    }
   ],
   "source": [
    "#2.3.2 .detach()\n",
    "q= torch.randn(3, requires_grad= True) #tensor requiring gradient\n",
    "print(q)\n",
    "r= q.detach()\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0115, -0.6556, -0.7330], requires_grad=True)\n",
      "tensor([2.0115, 1.3444, 1.2670])\n"
     ]
    }
   ],
   "source": [
    "#2.3.3 with torch.no_grad():\n",
    "        #operations\n",
    "q= torch.randn(3, requires_grad= True) #tensor requiring gradient\n",
    "print(q)\n",
    "with torch.no_grad():\n",
    "    r= q+2\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        therefore no grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 About .backward():\n",
    "- it sums up the gradient value everytime it is called by the same function(fn OR y (y or z or w in case of 2nd cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(2):\n",
    "    model_output= (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here gradient is incorrect in all steps except the first one. Therefore we need to empty the gradient after each step of optmization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After emptying gradient, the gradient is correct for every steps (very important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(2):\n",
    "    model_output= (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_() #empties gradient/ sets gradient value to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extra\n",
    "- using optimizer provided by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.ones(4, requires_grad=True)\n",
    "# optimizer = torch.optim.SGD(weights, lr=0.01) #optimize using Stochastic Gradient Descent\n",
    "# optimizer.step() #.stepp() -> used for iteration\n",
    "# optimizer.zero_grad() # clearing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conclusion:\n",
    "to calculate gradient:\n",
    "- requires_grad = True @ tensor(say x) wrt which gradient is calculated\n",
    "- call y.backward() to calculate gradient(dy/dx) and store in 'x.grad'\n",
    "- flush the 'x.grad' with x.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef7f9a8012d9131766e31894c279374cc63c73121ed4db3b9e67a294a4bf0e74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
